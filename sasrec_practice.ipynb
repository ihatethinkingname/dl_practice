{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPN2iruH5Mr7LoUT8Pu3FZr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihatethinkingname/dl_practice/blob/main/sasrec_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import os"
      ],
      "metadata": {
        "id": "4wC2rfdLGux3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ivaiiurKZqRz"
      },
      "outputs": [],
      "source": [
        "# Transformer practice part\n",
        "\n",
        "class EnD(nn.Module):\n",
        "  def __init__(self,encoder,decoder,src_embed,tgt_embed,generator):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.src_embed=src_embed\n",
        "    self.tgt_embed=tgt_embed\n",
        "    self.generator=generator\n",
        "\n",
        "  def forward(self,src,tgt,src_mask,tgt_mask):\n",
        "    return self.decoder(self.encoder(src,src_mask),src_mask,tgt,tgt_mask)\n",
        "\n",
        "  def encode(self,src,src_mask):\n",
        "    return self.encoder(self.src_embed(src),src_mask)\n",
        "\n",
        "  def decode(self,memory,src_mask,tgt,tgt_mask):\n",
        "    return self.decoder(memory,src_mask,self.tgt_embed(tgt),tgt_mask)\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model):\n",
        "    super().__init__()\n",
        "    self.lut=nn.Embedding(vocab_size,d_model,padding_idx=0)\n",
        "    self.d_model=d_model\n",
        "  @property\n",
        "  def weight(self):\n",
        "    return self.lut.weight\n",
        "  def forward(self,x):\n",
        "    return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self,d_model,max_len=5000,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.pos_embedding=nn.Embedding(max_len,d_model,padding_idx=0)\n",
        "    self.dropout=nn.Dropout(dropout_rate)\n",
        "  @property\n",
        "  def weight(self):\n",
        "    return self.pos_embedding.weight\n",
        "  def forward(self,x,zero_padding=True):\n",
        "    batch_size=x.shape[0]\n",
        "    seq_len=x.shape[1]\n",
        "    device=x.device\n",
        "    pos_idx=torch.arange(0,seq_len,device=device).unsqueeze(0).expand(batch_size,seq_len)\n",
        "    if zero_padding:\n",
        "      pad_mask=x.abs().sum(dim=-1)!=0\n",
        "      pos_idx=pos_idx*pad_mask.long()\n",
        "    x = x + self.pos_embedding(pos_idx)\n",
        "    return self.dropout(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self,d_model,vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj=nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return F.log_softmax(self.proj(x),dim=-1)\n",
        "\n",
        "\n",
        "class EnDcoder(nn.Module):\n",
        "  def __init__(self,layer,N):\n",
        "    super().__init__()\n",
        "    self.layers=clone(layer,N)\n",
        "    self.norm=nn.LayerNorm(layer.d_model)\n",
        "\n",
        "  def forward(self,x,src_mask,tgt=None,tgt_mask=None):\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,src_mask,tgt,tgt_mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "def clone(layer,N):\n",
        "  return nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
        "\n",
        "class EnDcoderLayer(nn.Module):\n",
        "  def __init__(self,self_attn,ffn,d_model,dropout_rate,src_attn=None):\n",
        "    super().__init__()\n",
        "    self.self_attn=self_attn\n",
        "    self.ffn=ffn\n",
        "    self.connector=clone(SublayerConnection(d_model,dropout_rate),2)\n",
        "    self.d_model=d_model\n",
        "    if src_attn is not None:\n",
        "      self.src_attn=src_attn\n",
        "      self.connector.append(SublayerConnection(d_model,dropout_rate))\n",
        "\n",
        "  def forward(self,x,src_mask,tgt=None,tgt_mask=None):\n",
        "    if isEncoder(tgt,tgt_mask):\n",
        "      x=self.connector[0](x,lambda x: self.self_attn(x,x,x,src_mask))\n",
        "    else:\n",
        "      m=x         # m is memoery\n",
        "      x=tgt        # We keep using x as the thing go through the network\n",
        "      x=self.connector[0](x,lambda x: self.self_attn(x,x,x,tgt_mask))\n",
        "      x=self.connector[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n",
        "    return self.connector[-1](x,self.ffn)\n",
        "\n",
        "def isEncoder(tgt,tgt_mask):\n",
        "    # Encoder model\n",
        "    if tgt is None and tgt_mask is None:\n",
        "      return True\n",
        "    # Decoder model\n",
        "    elif tgt is not None and tgt_mask is not None:\n",
        "      return False\n",
        "    # Wrong input\n",
        "    else:\n",
        "      raise ValueError(\"Encoder instance input error: \"\\\n",
        "               \"tgt and tgt_mask should be both None or both not None.\")\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self,d_model,dropout_rate):\n",
        "    super().__init__()\n",
        "    self.norm=nn.LayerNorm(d_model)\n",
        "    self.dropout=nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self,x,sublayer):\n",
        "    return x+self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "def attention(q,k,v,mask=None,dropout=None):\n",
        "  d_k=q.shape[-1]\n",
        "  scores=torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scores=scores.masked_fill(mask==0,-1e9)\n",
        "  p_attn=F.softmax(scores,dim=-1)\n",
        "  if dropout is not None:\n",
        "    p_attn=dropout(p_attn)\n",
        "  return torch.matmul(p_attn,v),p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self,n_head,d_model,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    assert d_model%n_head==0\n",
        "    self.d_head=d_model//n_head\n",
        "    self.n_head=n_head\n",
        "    self.linears=clone(nn.Linear(d_model,d_model),4)\n",
        "    self.p_attn=None\n",
        "    self.dropout=nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self,query,key,value,mask=None):\n",
        "    if mask is not None:\n",
        "      mask=mask.unsqueeze(1)\n",
        "    batch_size,seq_len,_=query.shape\n",
        "    q,k,v=[l(x).view(batch_size,seq_len,self.n_head,self.d_head).transpose(1,2)\n",
        "        for l,x in zip(self.linears,(query,key,value))]\n",
        "    x,self.p_attn=attention(q,k,v,mask=mask,dropout=self.dropout)\n",
        "    x=x.transpose(1,2).contiguous().view(batch_size,seq_len,self.n_head*self.d_head)\n",
        "    return self.linears[-1](x)\n",
        "\n",
        "class FFN(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.L1=nn.Linear(d_model,d_ff)\n",
        "    self.dropout=nn.Dropout(0.1)\n",
        "    self.L2=nn.Linear(d_ff,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.L2(self.dropout(F.relu(self.L1(x))))\n",
        "\n",
        "def causal_mask(seq_len,device):\n",
        "  # causal_mask=~torch.tril(torch.ones((seq_len,seq_len),dtype=torch.float,device=self.device))\n",
        "  i=torch.arange(seq_len,device=device)\n",
        "  return (i[:,None])>=(i[None,:])\n",
        "\n",
        "def make_model(src_vocab,tgt_vocab,d_model=512,n_sublayer=6,d_ff=2048,n_head=8,dropout_rate=0.1):\n",
        "  c=copy.deepcopy\n",
        "\n",
        "  src_embed=Embeddings(src_vocab,d_model)\n",
        "  tgt_embed=Embeddings(tgt_vocab,d_model)\n",
        "  generator=Generator(d_model,tgt_vocab)\n",
        "  pos_embed=PositionalEmbedding(d_model)\n",
        "\n",
        "  attn=MultiHeadedAttention(n_head,d_model,dropout_rate)\n",
        "  ffn=FFN(d_model,d_ff,dropout_rate)\n",
        "  encoder=EnDcoder(EnDcoderLayer(c(attn),c(ffn),d_model,dropout_rate),n_sublayer)\n",
        "  decoder=EnDcoder(EnDcoderLayer(c(attn),c(ffn),d_model,dropout_rate,c(attn)),n_sublayer)\n",
        "\n",
        "  model=EnD(encoder,decoder,nn.Sequential(c(src_embed),c(pos_embed)),\n",
        "            nn.Sequential(c(tgt_embed),c(pos_embed)),generator)\n",
        "\n",
        "  for p in model.parameters():\n",
        "    if p.dim()>1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SASRec practice part\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "  def __init__(self,n_user,n_item,args):\n",
        "    super().__init__()\n",
        "    self.n_user=n_user\n",
        "    self.n_item=n_item\n",
        "    self.device=args.device\n",
        "    d_model=args.d_model\n",
        "\n",
        "    self.item_emb=Embeddings(n_item,d_model)\n",
        "    self.pos_emb=PositionalEmbedding(d_model,args.max_len,args.dropout_rate)\n",
        "\n",
        "    self.self_attn=MultiHeadedAttention(args.n_head,d_model,args.dropout_rate)\n",
        "    self.ffn=FFN(d_model,args.d_ff,args.dropout_rate)\n",
        "    self.encoder=EnDcoder(EnDcoderLayer(self.self_attn,self.ffn,d_model,args.dropout_rate),args.n_sublayer)\n",
        "\n",
        "\n",
        "  def log2feat(self,log_seq):  # Is log_seq a tensor?\n",
        "    if not torch.is_tensor(log_seq):\n",
        "      log_seq = torch.as_tensor(log_seq)\n",
        "    log_seq = log_seq.to(self.device, dtype=torch.long)\n",
        "    pad_mask=(log_seq!=0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    log_seq=self.pos_emb(self.item_emb(log_seq))\n",
        "\n",
        "    seq_len=log_seq.shape[1]\n",
        "    a_causal_mask=causal_mask(seq_len,self.device).unsqueeze(0).unsqueeze(1)\n",
        "    combined_mask=pad_mask & a_causal_mask\n",
        "    feat_seq=self.encoder(log_seq,combined_mask)\n",
        "\n",
        "    return feat_seq\n",
        "\n",
        "  def forward(self,log_seq,pos_seq,neg_seq):  # Are pos_seq and neg_seq float?\n",
        "    feat_seq=self.log2feat(log_seq)\n",
        "\n",
        "    if not torch.is_tensor(pos_seq):\n",
        "        pos_seq = torch.as_tensor(pos_seq)\n",
        "    if not torch.is_tensor(neg_seq):\n",
        "        neg_seq = torch.as_tensor(neg_seq)\n",
        "    pos_seq_embed=self.item_emb(pos_seq.to(dtype=torch.long, device=self.device))\n",
        "    neg_seq_embed=self.item_emb(neg_seq.to(dtype=torch.long, device=self.device))\n",
        "\n",
        "    pos_logits=(feat_seq*pos_seq_embed).sum(dim=-1)\n",
        "    neg_logits=(feat_seq*neg_seq_embed).sum(dim=-1)\n",
        "\n",
        "    return pos_logits,neg_logits\n",
        "\n",
        "  def predict(self,log_seq,item_indices):\n",
        "    feat_seq=self.log2feat(log_seq)[:,-1,:]\n",
        "\n",
        "    if not torch.is_tensor(item_indices):\n",
        "      item_indices = torch.as_tensor(item_indices)\n",
        "    item_indices=item_indices.to(dtype=torch.long,device=self.device)\n",
        "\n",
        "    item_embed=self.item_emb(item_indices)\n",
        "\n",
        "    logits=item_embed.matmul(feat_seq.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def xavier_normal_init_parameters(self):\n",
        "    for name, param in self.named_parameters():\n",
        "      if param.dim()>1:\n",
        "        nn.init.xavier_normal_(param)\n",
        "    self.item_emb.weight.data[0,:]=0\n",
        "    self.pos_emb.weight.data[0,:]=0"
      ],
      "metadata": {
        "id": "rksJStJqG-YI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use comet for experiment management\n",
        "\n",
        "!pip install comet_ml > /dev/null 2>&1\n",
        "import comet_ml\n",
        "from google.colab import userdata\n",
        "COMET_API_KEY=userdata.get('comet_api_key')\n",
        "\n",
        "# Create a Comet experiment to track our training run\n",
        "def create_experiment(args):\n",
        "  # End any prior experiments\n",
        "  if 'experiment' in locals():\n",
        "    experiment.end()\n",
        "\n",
        "  # Initiate the comet experiment for tracking\n",
        "  experiment = comet_ml.Experiment(\n",
        "                  api_key=COMET_API_KEY,\n",
        "                  project_name=\"sasrec_practice\")\n",
        "\n",
        "  # Log our hyperparameters, defined above, to the experiment\n",
        "  for param, value in vars(args).items():\n",
        "    experiment.log_parameter(param, value)\n",
        "  experiment.flush()\n",
        "\n",
        "  return experiment"
      ],
      "metadata": {
        "id": "cdYVN1EfdU_u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install lion-pytorch > /dev/null 2>&1\n",
        "from lion_pytorch import Lion\n",
        "\n",
        "def download_from_github(file_url, local_filename=None):\n",
        "    if local_filename is None:\n",
        "        local_filename = file_url.split('/')[-1]\n",
        "\n",
        "    if os.path.exists(local_filename):\n",
        "        print(f\"文件已存在：{local_filename}，跳过下载\")\n",
        "    else:\n",
        "        print(f\"开始下载 {local_filename} ...\")\n",
        "        !wget -q {file_url}\n",
        "        print(f\"下载完成：{local_filename}\")\n",
        "\n",
        "def read_file(filepath='ml-1m.txt'):\n",
        "    user_hist = defaultdict(list)\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            user, item = line.strip().split()\n",
        "            user, item = int(user), int(item)\n",
        "            user_hist[user].append(item)\n",
        "    return user_hist\n",
        "\n",
        "# Actually, these three aren't needed, since the data has been processed\n",
        "def create_item_mapping(user_hist):\n",
        "    all_items = set()\n",
        "    for items in user_hist.values():\n",
        "        all_items.update(items)\n",
        "    item2idx = {item: idx + 1 for idx, item in enumerate(all_items)}  # 0 用于 PAD\n",
        "    idx2item = {idx: item for item, idx in item2idx.items()}\n",
        "    return item2idx, idx2item\n",
        "\n",
        "def create_user_mapping(user_hist):\n",
        "    all_users = sorted(user_hist.keys())\n",
        "    user2idx = {u: idx + 1 for idx, u in enumerate(all_users)}  # 1-based, 0 for padding if needed\n",
        "    idx2user = {idx: u for u, idx in user2idx.items()}\n",
        "    return user2idx, idx2user\n",
        "\n",
        "def make_consecutive_user_hist(user_hist, user2idx, item2idx):\n",
        "    new_user_hist = defaultdict(list)\n",
        "    for user, items in user_hist.items():\n",
        "        if user not in user2idx:  # Theoretically won't happen, just in case user_hist was changed\n",
        "            continue\n",
        "        new_u = user2idx[user]\n",
        "        new_items = [item2idx[i] for i in items if i in item2idx]\n",
        "        new_user_hist[new_u].extend(new_items)\n",
        "    return new_user_hist\n",
        "\n",
        "\n",
        "def data_partition(user_hist):\n",
        "  user_train = {}\n",
        "  user_valid = {}\n",
        "  user_test = {}\n",
        "  for user in user_hist:\n",
        "    n_feedback=len(user_hist[user])\n",
        "    if n_feedback<4:     # Change, n_feedback==3 would be discraded in SASRecDataset\n",
        "      user_train[user]=user_hist[user]\n",
        "      user_valid[user]=[]\n",
        "      user_test[user]=[]\n",
        "    else:\n",
        "      user_train[user]=user_hist[user][:-2]\n",
        "      user_valid[user]=[user_hist[user][-2]]\n",
        "      user_test[user]=[user_hist[user][-1]]\n",
        "  return user_train,user_valid,user_test\n",
        "\n",
        "def random_seq(l,r,s):\n",
        "  rand=np.random.randint(l,r)\n",
        "  while rand in s:\n",
        "    rand=np.random.randint(l,r)\n",
        "  return rand\n",
        "\n",
        "class SASRecDataset(Dataset):\n",
        "  def __init__(self,user_train,n_user,n_item,max_len):\n",
        "    self.user_train=user_train\n",
        "    self.n_user=n_user\n",
        "    self.n_item=n_item\n",
        "    self.max_len=max_len\n",
        "    self.users=[u for u in range(1,n_user+1) if len(user_train[u])>1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.users)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    uid=self.users[idx]\n",
        "    seq, pos, neg=(np.zeros(self.max_len,dtype=np.int32) for _ in range(3))\n",
        "\n",
        "    nxt=self.user_train[uid][-1]\n",
        "    idx=self.max_len-1\n",
        "    ts=set(self.user_train[uid])\n",
        "\n",
        "    for i in reversed(self.user_train[uid][:-1]):\n",
        "      seq[idx]=i\n",
        "      pos[idx]=nxt\n",
        "      neg[idx]=random_seq(1,self.n_item+1,ts) # Change, don't need \"if nxt != 0\"\n",
        "      nxt=i\n",
        "      idx-=1\n",
        "      if idx==-1:\n",
        "        break\n",
        "\n",
        "    uid, seq, pos, neg=(torch.tensor(x, dtype=torch.long) for x in (uid, seq, pos, neg))\n",
        "\n",
        "    return uid, seq, pos, neg\n",
        "\n",
        "class BPRLoss(nn.Module):\n",
        "  def forward(self,pos_logits,neg_logits):\n",
        "    return -torch.mean(F.logsigmoid(pos_logits-neg_logits))\n",
        "\n",
        "def share_eval(model,dataset,args,evaluate_test,criterion):\n",
        "  [train, valid, test, usernum, itemnum] = copy.deepcopy(dataset)\n",
        "\n",
        "  NDCG = 0.0\n",
        "  valid_user = 0.0\n",
        "  HT = 0.0\n",
        "  total_loss = 0.0\n",
        "\n",
        "  if usernum>10000:\n",
        "    users = random.sample(range(1, usernum + 1), 10000)\n",
        "  else:\n",
        "    users = range(1, usernum + 1)\n",
        "\n",
        "  for u in users:\n",
        "    if len(train[u]) < 1 or len(valid[u]) < 1: continue # Not necessary\n",
        "\n",
        "    seq = np.zeros([args.maxlen], dtype=np.int32)\n",
        "    idx = args.maxlen - 1\n",
        "    if evaluate_test:\n",
        "      seq[idx] = valid[u][0]\n",
        "      idx -= 1\n",
        "    for i in reversed(train[u]):\n",
        "        seq[idx] = i\n",
        "        idx -= 1\n",
        "        if idx == -1: break\n",
        "\n",
        "    rated = set(train[u])\n",
        "    rated.add(0)\n",
        "    item_idx = [valid[u][0]]\n",
        "    for _ in range(100):\n",
        "        t = np.random.randint(1, itemnum + 1)\n",
        "        while t in rated: t = np.random.randint(1, itemnum + 1)\n",
        "        item_idx.append(t)\n",
        "\n",
        "    seq_t = torch.tensor([seq], dtype=torch.long, device=args.device)\n",
        "    item_t = torch.tensor(item_idx, dtype=torch.long, device=args.device)\n",
        "\n",
        "    predictions = -model.predict(seq_t, item_t)\n",
        "    predictions = predictions[0]\n",
        "\n",
        "    rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "    valid_user += 1\n",
        "\n",
        "    if rank < 10:\n",
        "        NDCG += 1 / np.log2(rank + 2)\n",
        "        HT += 1\n",
        "\n",
        "    pos_logits = predictions[0]  # 第0个是正样本\n",
        "    neg_logits = predictions[1:] # 剩下的是负样本\n",
        "    # 转 tensor\n",
        "    pos_logits = torch.tensor([pos_logits], device=args.device)\n",
        "    neg_logits = torch.tensor(neg_logits, device=args.device)\n",
        "    loss = criterion(pos_logits, neg_logits)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if valid_user % 100 == 0:\n",
        "        print('.', end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "  return NDCG / valid_user, HT / valid_user, total_loss / valid_user\n",
        "\n",
        "def evaluate_valid(model,dataset,args):\n",
        "  return share_eval(model,dataset,args,evaluate_test=False,criterion=bpr_criterion)\n",
        "\n",
        "def evaluate(model,dataset,args):\n",
        "  return share_eval(model,dataset,args,evaluate_test=True,criterion=bpr_criterion)\n"
      ],
      "metadata": {
        "id": "yWzfd_Vwq1en"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "args=dict(\n",
        "    d_model=32,\n",
        "    n_head=2,\n",
        "    d_ff=4*32,\n",
        "    n_sublayer=2,\n",
        "    dropout_rate=0.5,\n",
        "\n",
        "    device='cuda:0',\n",
        "    inference_only=False,\n",
        "    n_epoch=4,\n",
        "    batch_size=64,\n",
        "    max_len=200,\n",
        "    l2_emb=1e-4,\n",
        "    lr=1e-4,\n",
        "    n_neg_sample=1,   # Use 1 negative sample first\n",
        "\n",
        "    top_k=10\n",
        ")\n",
        "args = argparse.Namespace(**args)"
      ],
      "metadata": {
        "id": "ZTE0D2j7yKWt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "file_url = \"https://raw.githubusercontent.com/pmixer/SASRec.pytorch/bdb6b77da98b9d6c395283d8b4e1a8bd7cb91efa/python/data/ml-1m.txt\"\n",
        "download_from_github(file_url)\n",
        "user_hist=read_file()\n",
        "\n",
        "# Data processing and preparing part\n",
        "\n",
        "# Not actually needed. This dataset has processed in \"User Item\" form with continuous int id\n",
        "user2idx, idx2user = create_user_mapping(user_hist)\n",
        "item2idx, idx2item = create_item_mapping(user_hist)\n",
        "user_hist = make_consecutive_user_hist(user_hist, user2idx, item2idx)\n",
        "n_user = len(user2idx)\n",
        "n_item = len(item2idx)\n",
        "\n",
        "user_train,user_valid,user_test = data_partition(user_hist)\n",
        "dataset = SASRecDataset(user_train, n_user, n_item, args.max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "cc = 0.0\n",
        "for u in user_train:\n",
        "    cc += len(user_train[u])\n",
        "print('average sequence length: %.2f' % (cc / len(user_train)))\n",
        "\n",
        "model=SASRec(n_user,n_item,args).to(args.device)\n",
        "model.xavier_normal_init_parameters()\n",
        "\n",
        "if args.inference_only:\n",
        "  model.eval()\n",
        "  t_test=evaluate(model,dataset,args)\n",
        "  print('test (NDCG@10: %.4f, HR@10: %.4f)' % (t_test[0], t_test[1]))\n",
        "\n",
        "model.train()\n",
        "\n",
        "\n",
        "# Train part\n",
        "bpr_criterion=BPRLoss()\n",
        "lion_optimizer=Lion(model.parameters(),lr=args.lr,betas=(0.9,0.99),weight_decay=0.0)\n",
        "\n",
        "best_val_ndcg, best_val_hr=0.0, 0.0\n",
        "\n",
        "history=defaultdict(list)\n",
        "plt.ion()\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "experiment=create_experiment(args)\n",
        "for epoch in range(args.n_epoch):\n",
        "  if args.inference_only: break\n",
        "  step_loss=[]\n",
        "  total_loss=0.0\n",
        "  t0=time.time()\n",
        "  for idx,batch in enumerate(tqdm(dataloader)):\n",
        "    lion_optimizer.zero_grad()\n",
        "    u,seq,pos,neg=(i.to(args.device) for i in batch)\n",
        "    pos_logits,neg_logits=model(seq,pos,neg)\n",
        "    mask=(pos!=0)\n",
        "    loss=bpr_criterion(pos_logits[mask],neg_logits[mask])\n",
        "    emb=model.item_emb.weight\n",
        "    loss=loss+args.l2_emb*torch.sum(emb**2)  # Change, use sum of squares\n",
        "    loss.backward()\n",
        "    lion_optimizer.step()\n",
        "    step_loss.append(loss.item())\n",
        "    total_loss += loss.item()\n",
        "    global_step=epoch*len(dataloader)+idx\n",
        "    experiment.log_metric(\"train_loss\",loss.item(),step=global_step)\n",
        "\n",
        "  # Evaluate part\n",
        "  model.eval()\n",
        "  t1=time.time()-t0\n",
        "  train_loss=total_loss/len(dataloader)\n",
        "  t_valid=evaluate_valid(model,dataset,args)\n",
        "  print('epoch:%d, time: %f(s), train_loss: %.4f, val_loss: %.4f \\n \\\n",
        "      valid (NDCG@10: %.4f, HR@10: %.4f)'\n",
        "      % (epoch, t1, train_loss, t_valid[2], t_valid[0], t_valid[1]))\n",
        "\n",
        "  experiment.log_metric(\"val_ndcg@10\", t_valid[0], step=epoch)\n",
        "  experiment.log_metric(\"val_hr@10\", t_valid[1], step=epoch)\n",
        "\n",
        "  history[\"train_loss_epoch\"].append(train_loss)\n",
        "  history[\"val_loss\"].append(t_valid[2])\n",
        "  history[\"ndcg\"].append(t_valid[0])\n",
        "  history[\"hr\"].append(t_valid[1])\n",
        "\n",
        "  ax1.clear()\n",
        "  ax1.plot(history[\"train_loss_epoch\"], label=\"Train Loss (epoch)\")\n",
        "  ax1.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend()\n",
        "  ax1.set_title(\"Loss Curve\")\n",
        "\n",
        "  ax2.clear()\n",
        "  ax2.plot(history[\"ndcg\"], label=\"NDCG@10\")\n",
        "  ax2.plot(history[\"hr\"], label=\"HR@10\")\n",
        "  ax2.set_xlabel(\"Epoch\")\n",
        "  ax2.set_ylabel(\"Metric\")\n",
        "  ax2.legend()\n",
        "  ax2.set_title(\"Validation Metrics\")\n",
        "\n",
        "  plt.pause(0.01)  # 关键：刷新图像\n",
        "  model.train()\n",
        "plt.ioff()\n",
        "plt.show()\n",
        "\n",
        "t_test = evaluate(model, dataset, args)\n",
        "print('test_loss: %.4f valid (NDCG@10: %.4f, HR@10: %.4f)'\n",
        "      % (t_test[2], t_test[0], t_test[1]))\n",
        "\n",
        "experiment.log_metric(\"test_loss\", t_test[2])\n",
        "experiment.log_metric(\"test_ndcg@10\", t_test[0])\n",
        "experiment.log_metric(\"test_hr@10\", t_test[1])\n",
        "\n",
        "# Cleanup stage\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "experiment.log_model(\"sasrec\", \"model.pth\")\n",
        "experiment.end()"
      ],
      "metadata": {
        "id": "aA5M8yvYGr4X",
        "outputId": "9467fcc4-6565-455d-f76a-21d06ca83851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件已存在：ml-1m.txt，跳过下载\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average sequence length: 163.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : moccasin_earwig_4732\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/richard-murphy/sasrec-practice/0fbf03f9a1e14e02a8cdf7fc333c8872\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=https%3A%2F%2Fgithub.com%2Fihatethinkingname%2Fdl_practice%2Fblob%2Fmain%2Fsasrec_practice.ipynb\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size     : 64\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     d_ff           : 128\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     d_model        : 32\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device         : cuda:0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate   : 0.5\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inference_only : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     l2_emb         : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr             : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_len        : 200\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_epoch        : 4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_head         : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_neg_sample   : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_sublayer     : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_k          : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/richard-murphy/sasrec-practice/615bb19a0a504aabb2af2f1fead25a4b\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m The process of logging environment details (conda environment, git patch) is underway. Please be patient as this may take some time.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 31 metrics, params and output messages\n",
            "  0%|          | 0/95 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[64, 200, 32]' is invalid for input of size 26214400",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3283232861.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mlion_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mpos_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbpr_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2825978086.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_seq, pos_seq, neg_seq)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Are pos_seq and neg_seq float?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mfeat_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2825978086.py\u001b[0m in \u001b[0;36mlog2feat\u001b[0;34m(self, log_seq)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0ma_causal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcombined_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_mask\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0ma_causal_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfeat_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcombined_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeat_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3054213693.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3054213693.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m         \u001b[0;31m# m is memoery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3054213693.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3054213693.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m         \u001b[0;31m# m is memoery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3054213693.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    141\u001b[0m         for l,x in zip(self.linears,(query,key,value))]\n\u001b[1;32m    142\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[64, 200, 32]' is invalid for input of size 26214400"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 生成假数据\n",
        "epochs = np.arange(1, 21)\n",
        "train_loss = np.linspace(1.0, 0.3, 20) + np.random.normal(0, 0.05, 20)\n",
        "val_loss = np.linspace(1.2, 0.4, 20) + np.random.normal(0, 0.05, 20)\n",
        "ndcg = np.linspace(0.2, 0.6, 20) + np.random.normal(0, 0.02, 20)\n",
        "hr = np.linspace(0.3, 0.8, 20) + np.random.normal(0, 0.02, 20)\n",
        "\n",
        "# 绘制子图\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# 损失曲线\n",
        "ax1.plot(epochs, train_loss, label=\"Train Loss\")\n",
        "ax1.plot(epochs, val_loss, label=\"Validation Loss\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.set_title(\"Training and Validation Loss\")\n",
        "ax1.legend()\n",
        "\n",
        "# 指标曲线\n",
        "ax2.plot(epochs, ndcg, label=\"NDCG@10\")\n",
        "ax2.plot(epochs, hr, label=\"HR@10\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Metric\")\n",
        "ax2.set_title(\"Evaluation Metrics\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "emPLW0xUc4E8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}