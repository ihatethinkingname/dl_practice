{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqCCKa4GWrhLykg8gZHupx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihatethinkingname/dl_practice/blob/main/sasrec_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import os"
      ],
      "metadata": {
        "id": "4wC2rfdLGux3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivaiiurKZqRz"
      },
      "outputs": [],
      "source": [
        "# Transformer practice part\n",
        "\n",
        "class EnD(nn.Module):\n",
        "  def __init__(self,encoder,decoder,src_embed,tgt_embed,generator):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.src_embed=src_embed\n",
        "    self.tgt_embed=tgt_embed\n",
        "    self.generator=generator\n",
        "\n",
        "  def forward(self,src,tgt,src_mask,tgt_mask):\n",
        "    return self.decoder(self.encoder(src,src_mask),src_mask,tgt,tgt_mask)\n",
        "\n",
        "  def encode(self,src,src_mask):\n",
        "    return self.encoder(self.src_embed(src),src_mask)\n",
        "\n",
        "  def decode(self,memory,src_mask,tgt,tgt_mask):\n",
        "    return self.decoder(memory,src_mask,self.tgt_embed(tgt),tgt_mask)\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model):\n",
        "    super().__init__()\n",
        "    self.lut=nn.Embedding(vocab_size,d_model,padding_idx=0)\n",
        "    self.d_model=d_model\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self,d_model,max_len=5000,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.pos_embedding=nn.Embedding(max_len,d_model)\n",
        "    self.dropout=nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self,x,zero_padding=True):\n",
        "    batch_size,seq_len=x.shape[:,2]\n",
        "    device=x.device\n",
        "    pos_idx=torch.arange(0,seq_len,device=device).unsqueeze(0).expand(batch_size,seq_len)\n",
        "    if zero_padding:\n",
        "      pad_mask=x.abs().sum(dim=-1)!=0\n",
        "      pos_idx*=pad_mask.long()\n",
        "    x = x + self.pos_embedding(pos_idx)\n",
        "    return self.dropout(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self,d_model,vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj=nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return F.log_softmax(self.proj(x),dim=-1)\n",
        "\n",
        "\n",
        "class EnDcoder(nn.Module):\n",
        "  def __init__(self,layer,N):\n",
        "    super().__init__()\n",
        "    self.layers=clone(layer,N)\n",
        "    self.norm=nn.LayerNorm(layer.d_model)\n",
        "\n",
        "  def forward(self,x,src_mask,tgt=None,tgt_mask=None):\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,src_mask,tgt,tgt_mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "def clone(layer,N):\n",
        "  return nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
        "\n",
        "class EnDcoderLayer(nn.Module):\n",
        "  def __init__(self,self_attn,ffn,d_model,dropout_rate,src_attn=None):\n",
        "    super().__init__()\n",
        "    self.self_attn=self_attn\n",
        "    self.ffn=ffn\n",
        "    self.connector=clone(SublayerConnection(d_model,dropout_rate),2)\n",
        "    self.d_model=d_model\n",
        "    if src_attn is not None:\n",
        "      self.src_attn=src_attn\n",
        "      self.connector.append(SublayerConnection(d_model,dropout_rate))\n",
        "\n",
        "  def forward(self,x,src_mask,tgt=None,tgt_mask=None):\n",
        "    if isEncoder(tgt,tgt_mask):\n",
        "      x=self.connector[0](x,lambda x: self.self_attn(x,x,x,src_mask))\n",
        "    else:\n",
        "      m=x         # m is memoery\n",
        "      x=tgt        # We keep using x as the thing go through the network\n",
        "      x=self.connector[0](x,lambda x: self.self_attn(x,x,x,tgt_mask))\n",
        "      x=self.connector[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n",
        "    return self.connector[-1](x,self.ffn)\n",
        "\n",
        "def isEncoder(tgt,tgt_mask):\n",
        "    # Encoder model\n",
        "    if tgt is None and tgt_mask is None:\n",
        "      return True\n",
        "    # Decoder model\n",
        "    elif tgt is not None and tgt_mask is not None:\n",
        "      return False\n",
        "    # Wrong input\n",
        "    else:\n",
        "      raise ValueError(\"Encoder instance input error: \"\\\n",
        "               \"tgt and tgt_mask should be both None or both not None.\")\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self,d_model,dropout_rate):\n",
        "    super().__init__()\n",
        "    self.norm=nn.LayerNorm(d_model)\n",
        "    self.dropout=nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self,x,sublayer):\n",
        "    return x+self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "def attention(q,k,v,mask=None,dropout=None):\n",
        "  d_k=q.shape[-1]\n",
        "  scores=torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scores=scores.masked_fill(mask==0,-1e9)\n",
        "  p_attn=F.softmax(scores,dim=-1)\n",
        "  if dropout is not None:\n",
        "    p_attn=dropout(p_attn)\n",
        "  return torch.matmul(p_attn,v),p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self,n_head,d_model,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    assert d_model%n_head==0\n",
        "    self.d_head=d_model//n_head\n",
        "    self.n_head=n_head\n",
        "    self.linears=clone(nn.Linear(d_model,d_model),4)\n",
        "    self.p_attn=None\n",
        "    self.dropout=nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self,quary,key,value,mask=None):\n",
        "    if mask is not None:\n",
        "      mask=mask.unsqueeze(1)\n",
        "    batch_size=quary.shape[0]\n",
        "    q,k,v=[l(x).view(batch_size,-1,self.n_head,self.d_head).transpose(1,2)\n",
        "        for l,x in zip(self.linears,(quary,key,value))]\n",
        "    x,self.p_attn=attention(q,k,v,mask=mask,dropout=self.dropout)\n",
        "    x=x.transpose(1,2).contiguous().view(batch_size,-1,self.n_head*self.d_head)\n",
        "    return self.linears[-1](x)\n",
        "\n",
        "class FFN(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.L1=nn.Linear(d_model,d_ff)\n",
        "    self.dropout=nn.Dropout(0.1)\n",
        "    self.L2=nn.Linear(d_ff,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.L2(self.dropout(F.relu(self.L1(x))))\n",
        "\n",
        "def causal_mask(seq_len,device):\n",
        "  # causal_mask=~torch.tril(torch.ones((seq_len,seq_len),dtype=torch.float,device=self.device))\n",
        "  i=torch.arange(seq_len,device=device)\n",
        "  return (i[:,None])>=(i[None,:])\n",
        "\n",
        "def make_model(src_vocab,tgt_vocab,d_model=512,n_sublayer=6,d_ff=2048,n_head=8,dropout_rate=0.1):\n",
        "  c=copy.deepcopy\n",
        "\n",
        "  src_embed=Embeddings(src_vocab,d_model)\n",
        "  tgt_embed=Embeddings(tgt_vocab,d_model)\n",
        "  generator=Generator(d_model,tgt_vocab)\n",
        "  pos_embed=PositionalEncoding(d_model)\n",
        "\n",
        "  attn=MultiHeadedAttention(n_head,d_model,dropout_rate)\n",
        "  ffn=FFN(d_model,d_ff,dropout_rate)\n",
        "  encoder=EnDcoder(EnDcoderLayer(c(attn),c(ffn),d_model,dropout_rate),n_sublayer)\n",
        "  decoder=EnDcoder(EnDcoderLayer(c(attn),c(ffn),d_model,dropout_rate,c(attn)),n_sublayer)\n",
        "\n",
        "  model=EnD(encoder,decoder,nn.Sequential(c(src_embed),c(pos_embed)),\n",
        "            nn.Sequential(c(tgt_embed),c(pos_embed)),generator)\n",
        "\n",
        "  for p in model.parameters():\n",
        "    if p.dim()>1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SASRec practice part\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "  def __init__(self,n_user,n_item,args):\n",
        "    self.n_user=n_user\n",
        "    self.n_item=n_item\n",
        "    self.device=args.device\n",
        "    d_model=args.d_model\n",
        "\n",
        "    self.item_emb=Embeddings(n_item,d_model)\n",
        "    self.pos_encode=PositionalEncoding(d_model,args.max_len,args.dropout_rate)\n",
        "\n",
        "    self.self_attn=MultiHeadedAttention(args.n_head,d_model,args.dropout_rate)\n",
        "    self.ffn=FFN(d_model,args.d_ff,args.dropout_rate)\n",
        "    self.encoder=EnDcoder(EnDcoderLayer(self.self_attn,self.ffn,d_model,args.dropout_rate),args.n_sublayer)\n",
        "\n",
        "\n",
        "  def log2feat(self,log_seq):  # Is log_seq a tensor?\n",
        "    if not torch.is_tensor(log_seq):\n",
        "      log_seq = torch.as_tensor(log_seq)\n",
        "    log_seq = log_seq.to(self.device, dtype=torch.long)\n",
        "    pad_mask=(log_seq!=0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    log_seq=self.pos_encode(self.item_emb(log_seq))\n",
        "\n",
        "    seq_len=log_seq.shape[1]\n",
        "    causal_mask=causal_mask(seq_len,self.device).unsqueeze(0).unsqueeze(1)\n",
        "    combined_mask=pad_mask & causal_mask\n",
        "    feat_seq=self.encoder(log_seq,combined_mask)\n",
        "\n",
        "    return feat_seq\n",
        "\n",
        "  def forward(self,log_seq,pos_seq,neg_seq):  # Are pos_seq and neg_seq float?\n",
        "    feat_seq=self.log2feat(log_seq)\n",
        "\n",
        "    if not torch.is_tensor(pos_seq):\n",
        "        pos_seq = torch.as_tensor(pos_seq)\n",
        "    if not torch.is_tensor(neg_seq):\n",
        "        neg_seq = torch.as_tensor(neg_seq)\n",
        "    pos_seq_embed=self.item_emb(pos_seq.to(dtype=torch.long, device=self.device))\n",
        "    neg_seq_embed=self.item_emb(neg_seq.to(dtype=torch.long, device=self.device))\n",
        "\n",
        "    pos_logits=(feat_seq*pos_seq_embed).sum(dim=-1)\n",
        "    neg_logits=(feat_seq*neg_seq_embed).sum(dim=-1)\n",
        "\n",
        "    return pos_logits,neg_logits\n",
        "\n",
        "  def predict(self,log_seq,item_indices):\n",
        "    feat_seq=self.log2feat(log_seq)[:,-1,:]\n",
        "\n",
        "    if not torch.is_tensor(item_indices):\n",
        "      item_indices = torch.as_tensor(item_indices)\n",
        "    item_indices=item_indices.to(dtype=torch.long,device=self.device)\n",
        "\n",
        "    item_embed=self.item_emb(item_indices)\n",
        "\n",
        "    logits=item_embed.matmul(feat_seq.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "rksJStJqG-YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[1,2]\n"
      ],
      "metadata": {
        "id": "GellfP3ESq7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SASRec use ml-1m\n",
        "\n",
        "# 1. ä¸‹è½½\n",
        "if not os.path.exists(\"ml-1m.zip\"):\n",
        "    !wget http://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
        "else:\n",
        "    print(\"âœ… å·²å­˜åœ¨ ml-1m.zipï¼Œè·³è¿‡ä¸‹è½½\")\n",
        "\n",
        "# 2. è§£å‹\n",
        "if not os.path.exists(\"ml-1m\"):\n",
        "    !unzip -q ml-1m.zip\n",
        "    print(\"âœ… è§£å‹å®Œæˆ\")\n",
        "else:\n",
        "    print(\"âœ… å·²å­˜åœ¨ ml-1m æ–‡ä»¶å¤¹ï¼Œè·³è¿‡è§£å‹\")\n",
        "\n",
        "# 2. è¯»å– ratings.dat\n",
        "ratings = pd.read_csv(\n",
        "    \"ml-1m/ratings.dat\",\n",
        "    sep=\"::\",\n",
        "    engine=\"python\",\n",
        "    names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"],\n",
        "    encoding=\"latin-1\"\n",
        ")\n",
        "\n",
        "# 3. è¯»å– movies.dat\n",
        "movies = pd.read_csv(\n",
        "    \"ml-1m/movies.dat\",\n",
        "    sep=\"::\",\n",
        "    engine=\"python\",\n",
        "    names=[\"MovieID\", \"Title\", \"Genres\"],\n",
        "    encoding=\"latin-1\"\n",
        ")\n",
        "\n",
        "# 4. è¯»å– users.dat\n",
        "users = pd.read_csv(\n",
        "    \"ml-1m/users.dat\",\n",
        "    sep=\"::\",\n",
        "    engine=\"python\",\n",
        "    names=[\"UserID\", \"Gender\", \"Age\", \"Occupation\", \"Zip-code\"],\n",
        "    encoding=\"latin-1\"\n",
        ")\n",
        "\n",
        "# 5. æ•°æ®ç®€å•æŸ¥çœ‹\n",
        "print(\"ğŸ“Œ Ratings æ ·ä¾‹:\")\n",
        "print(ratings.head(), \"\\n\")\n",
        "\n",
        "print(\"ğŸ“Œ Movies æ ·ä¾‹:\")\n",
        "print(movies.head(), \"\\n\")\n",
        "\n",
        "print(\"ğŸ“Œ Users æ ·ä¾‹:\")\n",
        "print(users.head(), \"\\n\")\n",
        "\n",
        "# 6. ç»Ÿè®¡ä¿¡æ¯\n",
        "print(f\"ç”¨æˆ·æ•°é‡: {ratings['UserID'].nunique()}\")\n",
        "print(f\"ç”µå½±æ•°é‡: {ratings['MovieID'].nunique()}\")\n",
        "print(f\"è¯„åˆ†æ•°é‡: {len(ratings)}\")\n",
        "print(f\"è¯„åˆ†èŒƒå›´: {ratings['Rating'].min()} ~ {ratings['Rating'].max()}\")\n",
        "\n",
        "# 7. åˆå¹¶æˆä¸€ä¸ªå¤§è¡¨ï¼ˆæ–¹ä¾¿åˆ†æï¼‰\n",
        "df = ratings.merge(movies, on=\"MovieID\").merge(users, on=\"UserID\")\n",
        "print(\"\\nğŸ“Œ åˆå¹¶åçš„æ ·ä¾‹:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqDMIyxuQBRz",
        "outputId": "edfdc614-1ec7-4b8b-a47f-c978d19f70e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… å·²å­˜åœ¨ ml-1m.zipï¼Œè·³è¿‡ä¸‹è½½\n",
            "âœ… å·²å­˜åœ¨ ml-1m æ–‡ä»¶å¤¹ï¼Œè·³è¿‡è§£å‹\n",
            "ğŸ“Œ Ratings æ ·ä¾‹:\n",
            "   UserID  MovieID  Rating  Timestamp\n",
            "0       1     1193       5  978300760\n",
            "1       1      661       3  978302109\n",
            "2       1      914       3  978301968\n",
            "3       1     3408       4  978300275\n",
            "4       1     2355       5  978824291 \n",
            "\n",
            "ğŸ“Œ Movies æ ·ä¾‹:\n",
            "   MovieID                               Title                        Genres\n",
            "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
            "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
            "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
            "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
            "4        5  Father of the Bride Part II (1995)                        Comedy \n",
            "\n",
            "ğŸ“Œ Users æ ·ä¾‹:\n",
            "   UserID Gender  Age  Occupation Zip-code\n",
            "0       1      F    1          10    48067\n",
            "1       2      M   56          16    70072\n",
            "2       3      M   25          15    55117\n",
            "3       4      M   45           7    02460\n",
            "4       5      M   25          20    55455 \n",
            "\n",
            "ç”¨æˆ·æ•°é‡: 6040\n",
            "ç”µå½±æ•°é‡: 3706\n",
            "è¯„åˆ†æ•°é‡: 1000209\n",
            "è¯„åˆ†èŒƒå›´: 1 ~ 5\n",
            "\n",
            "ğŸ“Œ åˆå¹¶åçš„æ ·ä¾‹:\n",
            "   UserID  MovieID  Rating  Timestamp                                   Title  \\\n",
            "0       1     1193       5  978300760  One Flew Over the Cuckoo's Nest (1975)   \n",
            "1       1      661       3  978302109        James and the Giant Peach (1996)   \n",
            "2       1      914       3  978301968                     My Fair Lady (1964)   \n",
            "3       1     3408       4  978300275                  Erin Brockovich (2000)   \n",
            "4       1     2355       5  978824291                    Bug's Life, A (1998)   \n",
            "\n",
            "                         Genres Gender  Age  Occupation Zip-code  \n",
            "0                         Drama      F    1          10    48067  \n",
            "1  Animation|Children's|Musical      F    1          10    48067  \n",
            "2               Musical|Romance      F    1          10    48067  \n",
            "3                         Drama      F    1          10    48067  \n",
            "4   Animation|Children's|Comedy      F    1          10    48067  \n"
          ]
        }
      ]
    }
  ]
}