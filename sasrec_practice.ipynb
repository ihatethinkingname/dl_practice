{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAGLSf6bIe8L11oREcMlQh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihatethinkingname/dl_practice/blob/main/sasrec_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import copy\n",
        "import math\n"
      ],
      "metadata": {
        "id": "4wC2rfdLGux3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ivaiiurKZqRz"
      },
      "outputs": [],
      "source": [
        "# Transformer practice part\n",
        "\n",
        "class EnD(nn.Module):\n",
        "  def __init__(self,encoder,decoder,src_embed,tgt_embed,generator):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.src_embed=src_embed\n",
        "    self.tgt_embed=tgt_embed\n",
        "    self.generator=generator\n",
        "\n",
        "  def forward(self,src,tgt,src_mask,tgt_mask):\n",
        "    return self.decoder(self.encoder(src,src_mask),src_mask,tgt,tgt_mask)\n",
        "\n",
        "  def encode(self,src,src_mask):\n",
        "    return self.encoder(self.src_embed(src),src_mask)\n",
        "\n",
        "  def decode(self,memory,src_mask,tgt,tgt_mask):\n",
        "    return self.decoder(memory,src_mask,self.tgt_embed(tgt),tgt_mask)\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self,vocab_size,d_model):\n",
        "    super().__init__()\n",
        "    self.lut=nn.Embedding(vocab_size,d_model,padding_idx=0)\n",
        "    self.d_model=d_model\n",
        "  @property\n",
        "  def weight(self):\n",
        "    return self.lut.weight\n",
        "  def forward(self,x):\n",
        "    return self.lut(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "  def __init__(self,d_model,max_len=5000,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.pos_embedding=nn.Embedding(max_len,d_model,padding_idx=0)\n",
        "    self.dropout=nn.Dropout(dropout_rate)\n",
        "  @property\n",
        "  def weight(self):\n",
        "    return self.pos_embedding.weight\n",
        "  def forward(self,x,zero_padding=True):\n",
        "    batch_size=x.shape[0]\n",
        "    seq_len=x.shape[1]\n",
        "    device=x.device\n",
        "    pos_idx=torch.arange(0,seq_len,device=device).unsqueeze(0).expand(batch_size,seq_len)\n",
        "    if zero_padding:\n",
        "      pad_mask=x.abs().sum(dim=-1)!=0\n",
        "      pos_idx=pos_idx*pad_mask.long()\n",
        "    x = x + self.pos_embedding(pos_idx)\n",
        "    return self.dropout(x)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self,d_model,vocab_size):\n",
        "    super().__init__()\n",
        "    self.proj=nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return F.log_softmax(self.proj(x),dim=-1)\n",
        "\n",
        "\n",
        "class EnDcoder(nn.Module):\n",
        "  def __init__(self,layer,N):\n",
        "    super().__init__()\n",
        "    self.layers=clone(layer,N)\n",
        "    self.norm=nn.LayerNorm(layer.d_model)\n",
        "\n",
        "  def forward(self,x,src_mask,tgt=None,tgt_mask=None):\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,src_mask,tgt,tgt_mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "def clone(layer,N):\n",
        "  return nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
        "\n",
        "class EnDcoderLayer(nn.Module):\n",
        "  def __init__(self,self_attn,ffn,d_model,dropout_rate,src_attn=None):\n",
        "    super().__init__()\n",
        "    self.self_attn=self_attn\n",
        "    self.ffn=ffn\n",
        "    self.connector=clone(SublayerConnection(d_model,dropout_rate),2)\n",
        "    self.d_model=d_model\n",
        "    if src_attn is not None:\n",
        "      self.src_attn=src_attn\n",
        "      self.connector.append(SublayerConnection(d_model,dropout_rate))\n",
        "\n",
        "  def forward(self,x,src_mask,tgt=None,tgt_mask=None):\n",
        "    if isEncoder(tgt,tgt_mask):\n",
        "      x=self.connector[0](x,lambda x: self.self_attn(x,x,x,src_mask))\n",
        "    else:\n",
        "      m=x         # m is memoery\n",
        "      x=tgt        # We keep using x as the thing go through the network\n",
        "      x=self.connector[0](x,lambda x: self.self_attn(x,x,x,tgt_mask))\n",
        "      x=self.connector[1](x,lambda x: self.src_attn(x,m,m,src_mask))\n",
        "    return self.connector[-1](x,self.ffn)\n",
        "\n",
        "def isEncoder(tgt,tgt_mask):\n",
        "    # Encoder model\n",
        "    if tgt is None and tgt_mask is None:\n",
        "      return True\n",
        "    # Decoder model\n",
        "    elif tgt is not None and tgt_mask is not None:\n",
        "      return False\n",
        "    # Wrong input\n",
        "    else:\n",
        "      raise ValueError(\"Encoder instance input error: \"\\\n",
        "               \"tgt and tgt_mask should be both None or both not None.\")\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "  def __init__(self,d_model,dropout_rate):\n",
        "    super().__init__()\n",
        "    self.norm=nn.LayerNorm(d_model)\n",
        "    self.dropout=nn.Dropout(dropout_rate)\n",
        "\n",
        "  def forward(self,x,sublayer):\n",
        "    return x+self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "def attention(q,k,v,mask=None,dropout=None):\n",
        "  d_k=q.shape[-1]\n",
        "  scores=torch.matmul(q,k.transpose(-2,-1))/math.sqrt(d_k)\n",
        "  if mask is not None:\n",
        "    scores=scores.masked_fill(mask==0,-1e9)\n",
        "  p_attn=F.softmax(scores,dim=-1)\n",
        "  if dropout is not None:\n",
        "    p_attn=dropout(p_attn)\n",
        "  return torch.matmul(p_attn,v),p_attn\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self,n_head,d_model,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    assert d_model%n_head==0\n",
        "    self.d_head=d_model//n_head\n",
        "    self.n_head=n_head\n",
        "    self.linears=clone(nn.Linear(d_model,d_model),4)\n",
        "    self.p_attn=None\n",
        "    self.dropout=nn.Dropout(p=dropout_rate)\n",
        "\n",
        "  def forward(self,query,key,value,mask=None):\n",
        "    if mask is not None:\n",
        "      if mask.dim() == 3:\n",
        "        mask = mask.unsqueeze(1)  # (batch,1,seq_len,seq_len)\n",
        "        print(\"org mask dim is 3\")\n",
        "      elif mask.dim() == 4 and mask.size(1) != 1:\n",
        "        print(\"fine\")\n",
        "        raise ValueError(f\"mask shape {mask.shape} 不符合要求\")\n",
        "    batch_size,seq_len,d_model=query.shape\n",
        "    assert d_model==self.n_head*self.d_head\n",
        "    q,k,v=[l(x).view(batch_size,seq_len,self.n_head,self.d_head).transpose(1,2)\n",
        "        for l,x in zip(self.linears,(query,key,value))]\n",
        "    x,self.p_attn=attention(q,k,v,mask=mask,dropout=self.dropout)\n",
        "    x=x.transpose(1,2).contiguous().view(batch_size,seq_len,self.n_head*self.d_head)\n",
        "\n",
        "    return self.linears[-1](x)\n",
        "\n",
        "class FFN(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.L1=nn.Linear(d_model,d_ff)\n",
        "    self.dropout=nn.Dropout(0.1)\n",
        "    self.L2=nn.Linear(d_ff,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.L2(self.dropout(F.relu(self.L1(x))))\n",
        "\n",
        "def causal_mask(seq_len,device):\n",
        "  # causal_mask=~torch.tril(torch.ones((seq_len,seq_len),dtype=torch.float,device=self.device))\n",
        "  i=torch.arange(seq_len,device=device)\n",
        "  return (i[:,None])>=(i[None,:])\n",
        "\n",
        "def make_model(src_vocab,tgt_vocab,d_model=512,n_sublayer=6,d_ff=2048,n_head=8,dropout_rate=0.1):\n",
        "  c=copy.deepcopy\n",
        "\n",
        "  src_embed=Embeddings(src_vocab,d_model)\n",
        "  tgt_embed=Embeddings(tgt_vocab,d_model)\n",
        "  generator=Generator(d_model,tgt_vocab)\n",
        "  pos_embed=PositionalEmbedding(d_model)\n",
        "\n",
        "  attn=MultiHeadedAttention(n_head,d_model,dropout_rate)\n",
        "  ffn=FFN(d_model,d_ff,dropout_rate)\n",
        "  encoder=EnDcoder(EnDcoderLayer(c(attn),c(ffn),d_model,dropout_rate),n_sublayer)\n",
        "  decoder=EnDcoder(EnDcoderLayer(c(attn),c(ffn),d_model,dropout_rate,c(attn)),n_sublayer)\n",
        "\n",
        "  model=EnD(encoder,decoder,nn.Sequential(c(src_embed),c(pos_embed)),\n",
        "            nn.Sequential(c(tgt_embed),c(pos_embed)),generator)\n",
        "\n",
        "  for p in model.parameters():\n",
        "    if p.dim()>1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SASRec practice part\n",
        "\n",
        "class SASRec(nn.Module):\n",
        "  def __init__(self,n_user,n_item,args):\n",
        "    super().__init__()\n",
        "    self.n_user=n_user\n",
        "    self.n_item=n_item\n",
        "    self.device=args.device\n",
        "    d_model=args.d_model\n",
        "\n",
        "    self.item_emb=Embeddings(n_item+1,d_model)\n",
        "    self.pos_emb=PositionalEmbedding(d_model,args.max_len,args.dropout_rate)\n",
        "\n",
        "    self.self_attn=MultiHeadedAttention(args.n_head,d_model,args.dropout_rate)\n",
        "    self.ffn=FFN(d_model,args.d_ff,args.dropout_rate)\n",
        "    self.encoder=EnDcoder(EnDcoderLayer(self.self_attn,self.ffn,d_model,args.dropout_rate),args.n_sublayer)\n",
        "\n",
        "\n",
        "  def log2feat(self,log_seq):  # Is log_seq a tensor?\n",
        "    if not torch.is_tensor(log_seq):\n",
        "      log_seq = torch.as_tensor(log_seq)\n",
        "    log_seq = log_seq.to(self.device, dtype=torch.long)\n",
        "    pad_mask=(log_seq!=0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    log_seq=self.pos_emb(self.item_emb(log_seq))\n",
        "\n",
        "    seq_len=log_seq.shape[1]\n",
        "    a_causal_mask=causal_mask(seq_len,self.device).unsqueeze(0).unsqueeze(1)\n",
        "    combined_mask=pad_mask & a_causal_mask\n",
        "    feat_seq=self.encoder(log_seq,combined_mask)\n",
        "\n",
        "    return feat_seq\n",
        "\n",
        "  def forward(self,log_seq,pos_seq,neg_seq):  # Are pos_seq and neg_seq float?\n",
        "    feat_seq=self.log2feat(log_seq)\n",
        "\n",
        "    if not torch.is_tensor(pos_seq):\n",
        "        pos_seq = torch.as_tensor(pos_seq)\n",
        "    if not torch.is_tensor(neg_seq):\n",
        "        neg_seq = torch.as_tensor(neg_seq)\n",
        "    pos_seq_embed=self.item_emb(pos_seq.to(dtype=torch.long, device=self.device))\n",
        "    neg_seq_embed=self.item_emb(neg_seq.to(dtype=torch.long, device=self.device))\n",
        "\n",
        "    pos_logits=(feat_seq*pos_seq_embed).sum(dim=-1)\n",
        "    neg_logits=(feat_seq*neg_seq_embed).sum(dim=-1)\n",
        "\n",
        "    return pos_logits,neg_logits\n",
        "\n",
        "  def predict(self,log_seq,item_indices):\n",
        "    feat_seq=self.log2feat(log_seq)[:,-1,:]\n",
        "\n",
        "    if not torch.is_tensor(item_indices):\n",
        "      item_indices = torch.as_tensor(item_indices)\n",
        "    item_indices=item_indices.to(dtype=torch.long,device=self.device)\n",
        "\n",
        "    item_embed=self.item_emb(item_indices)\n",
        "\n",
        "    logits=item_embed.matmul(feat_seq.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def xavier_normal_init_parameters(self):\n",
        "    for name, param in self.named_parameters():\n",
        "      if param.dim()>1:\n",
        "        nn.init.xavier_normal_(param)\n",
        "    self.item_emb.weight.data[0,:]=0\n",
        "    self.pos_emb.weight.data[0,:]=0"
      ],
      "metadata": {
        "id": "rksJStJqG-YI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use comet for experiment management\n",
        "\n",
        "!pip install comet_ml > /dev/null 2>&1\n",
        "import comet_ml\n",
        "from google.colab import userdata\n",
        "COMET_API_KEY=userdata.get('comet_api_key')\n",
        "\n",
        "# Create a Comet experiment to track our training run\n",
        "def create_experiment(args):\n",
        "  # End any prior experiments\n",
        "  if 'experiment' in locals():\n",
        "    experiment.end()\n",
        "\n",
        "  # Initiate the comet experiment for tracking\n",
        "  experiment = comet_ml.Experiment(\n",
        "                  api_key=COMET_API_KEY,\n",
        "                  project_name=\"sasrec_practice\")\n",
        "\n",
        "  # Log our hyperparameters, defined above, to the experiment\n",
        "  for param, value in vars(args).items():\n",
        "    experiment.log_parameter(param, value)\n",
        "  experiment.flush()\n",
        "\n",
        "  return experiment"
      ],
      "metadata": {
        "id": "cdYVN1EfdU_u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install lion-pytorch > /dev/null 2>&1\n",
        "from lion_pytorch import Lion\n",
        "\n",
        "def download_from_github(file_url, local_filename=None):\n",
        "    if local_filename is None:\n",
        "        local_filename = file_url.split('/')[-1]\n",
        "\n",
        "    if os.path.exists(local_filename):\n",
        "        print(f\"文件已存在：{local_filename}，跳过下载\")\n",
        "    else:\n",
        "        print(f\"开始下载 {local_filename} ...\")\n",
        "        !wget -q {file_url}\n",
        "        print(f\"下载完成：{local_filename}\")\n",
        "\n",
        "def read_file(filepath='ml-1m.txt'):\n",
        "    user_hist = defaultdict(list)\n",
        "    with open(filepath, 'r') as f:\n",
        "        for line in f:\n",
        "            user, item = line.strip().split()\n",
        "            user, item = int(user), int(item)\n",
        "            user_hist[user].append(item)\n",
        "    return user_hist\n",
        "\n",
        "# Actually, these three aren't needed, since the data has been processed\n",
        "def create_item_mapping(user_hist):\n",
        "    all_items = set()\n",
        "    for items in user_hist.values():\n",
        "        all_items.update(items)\n",
        "    item2idx = {item: idx + 1 for idx, item in enumerate(all_items)}  # 0 用于 PAD\n",
        "    idx2item = {idx: item for item, idx in item2idx.items()}\n",
        "    return item2idx, idx2item\n",
        "\n",
        "def create_user_mapping(user_hist):\n",
        "    all_users = sorted(user_hist.keys())\n",
        "    user2idx = {u: idx + 1 for idx, u in enumerate(all_users)}  # 1-based, 0 for padding if needed\n",
        "    idx2user = {idx: u for u, idx in user2idx.items()}\n",
        "    return user2idx, idx2user\n",
        "\n",
        "def make_consecutive_user_hist(user_hist, user2idx, item2idx):\n",
        "    new_user_hist = defaultdict(list)\n",
        "    for user, items in user_hist.items():\n",
        "        if user not in user2idx:  # Theoretically won't happen, just in case user_hist was changed\n",
        "            continue\n",
        "        new_u = user2idx[user]\n",
        "        new_items = [item2idx[i] for i in items if i in item2idx]\n",
        "        new_user_hist[new_u].extend(new_items)\n",
        "    return new_user_hist\n",
        "\n",
        "\n",
        "def data_partition(user_hist):\n",
        "  user_train = {}\n",
        "  user_valid = {}\n",
        "  user_test = {}\n",
        "  for user in user_hist:\n",
        "    n_feedback=len(user_hist[user])\n",
        "    if n_feedback<4:     # Change, n_feedback==3 would be discraded in SASRecDataset\n",
        "      user_train[user]=user_hist[user]\n",
        "      user_valid[user]=[]\n",
        "      user_test[user]=[]\n",
        "    else:\n",
        "      user_train[user]=user_hist[user][:-2]\n",
        "      user_valid[user]=[user_hist[user][-2]]\n",
        "      user_test[user]=[user_hist[user][-1]]\n",
        "  return user_train,user_valid,user_test\n",
        "\n",
        "def random_seq(l,r,s):\n",
        "  rand=np.random.randint(l,r)\n",
        "  while rand in s:\n",
        "    rand=np.random.randint(l,r)\n",
        "  return rand\n",
        "\n",
        "class SASRecDataset(Dataset):\n",
        "  def __init__(self,user_train,n_user,n_item,max_len):\n",
        "    self.user_train=user_train\n",
        "    self.n_user=n_user\n",
        "    self.n_item=n_item\n",
        "    self.max_len=max_len\n",
        "    self.users=[u for u in range(1,n_user+1) if len(user_train[u])>1]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.users)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    uid=self.users[idx]\n",
        "    seq, pos, neg=(np.zeros(self.max_len,dtype=np.int32) for _ in range(3))\n",
        "\n",
        "    nxt=self.user_train[uid][-1]\n",
        "    idx=self.max_len-1\n",
        "    ts=set(self.user_train[uid])\n",
        "\n",
        "    for i in reversed(self.user_train[uid][:-1]):\n",
        "      seq[idx]=i\n",
        "      pos[idx]=nxt\n",
        "      neg[idx]=random_seq(1,self.n_item+1,ts) # Change, don't need \"if nxt != 0\"\n",
        "      nxt=i\n",
        "      idx-=1\n",
        "      if idx==-1:\n",
        "        break\n",
        "\n",
        "    uid, seq, pos, neg=(torch.tensor(x, dtype=torch.long) for x in (uid, seq, pos, neg))\n",
        "\n",
        "    return uid, seq, pos, neg\n",
        "\n",
        "class BPRLoss(nn.Module):\n",
        "  def forward(self,pos_logits,neg_logits):\n",
        "    return -torch.mean(F.logsigmoid(pos_logits-neg_logits))\n",
        "\n",
        "def share_eval(model,dataset,args,evaluate_test,criterion):\n",
        "  [train, valid, test] = copy.deepcopy(dataset)\n",
        "\n",
        "  # Maybe better way\n",
        "  all_users = set(user_train.keys()) | set(user_valid.keys()) | set(user_test.keys())\n",
        "  usernum = max(all_users)   # 如果用户 id 从 1 开始连续\n",
        "  # usernum = len(all_users) # 如果你把用户重新映射过\n",
        "\n",
        "  # 2. 物品 id\n",
        "  all_items = set()\n",
        "  for dataset in (user_train, user_valid, user_test):\n",
        "      for items in dataset.values():\n",
        "          all_items.update(items)\n",
        "  itemnum = max(all_items)\n",
        "\n",
        "\n",
        "  NDCG = 0.0\n",
        "  valid_user = 0.0\n",
        "  HT = 0.0\n",
        "  total_loss = 0.0\n",
        "\n",
        "  if usernum>10000:\n",
        "    users = random.sample(range(1, usernum + 1), 10000)\n",
        "  else:\n",
        "    users = range(1, usernum + 1)\n",
        "\n",
        "  for u in users:\n",
        "    if len(train[u]) < 1 or len(valid[u]) < 1: continue # Not necessary\n",
        "\n",
        "    seq = np.zeros([args.max_len], dtype=np.int32)\n",
        "    idx = args.max_len - 1\n",
        "    if evaluate_test:\n",
        "      seq[idx] = valid[u][0]\n",
        "      idx -= 1\n",
        "    for i in reversed(train[u]):\n",
        "        seq[idx] = i\n",
        "        idx -= 1\n",
        "        if idx == -1: break\n",
        "\n",
        "    rated = set(train[u])\n",
        "    rated.add(0)\n",
        "    item_idx = [valid[u][0]]\n",
        "    for _ in range(100):\n",
        "        t = np.random.randint(1, itemnum + 1)\n",
        "        while t in rated: t = np.random.randint(1, itemnum + 1)\n",
        "        item_idx.append(t)\n",
        "\n",
        "    seq_t = torch.tensor([seq], dtype=torch.long, device=args.device)\n",
        "    item_t = torch.tensor(item_idx, dtype=torch.long, device=args.device)\n",
        "\n",
        "    # predictions = -model.predict(seq_t, item_t)\n",
        "    # predictions = predictions[0]\n",
        "\n",
        "    # rank = predictions.argsort().argsort()[0].item()\n",
        "\n",
        "    predictions = model.predict(seq_t, item_t)\n",
        "    predictions = predictions[0]\n",
        "\n",
        "    rank = predictions.argsort(descending=True).argsort()[0].item()\n",
        "\n",
        "    valid_user += 1\n",
        "\n",
        "    if rank < 10:\n",
        "        NDCG += 1 / np.log2(rank + 2)\n",
        "        HT += 1\n",
        "\n",
        "    pos_logits = predictions[0:1]  # 第0个是正样本\n",
        "    neg_logits = predictions[1:] # 剩下的是负样本\n",
        "    # 转 tensor\n",
        "    pos_logits = torch.tensor(pos_logits, device=args.device)\n",
        "    neg_logits = torch.tensor(neg_logits, device=args.device)\n",
        "    loss = criterion(pos_logits, neg_logits)\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    if valid_user % 100 == 0:\n",
        "        print('.', end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "  return NDCG / valid_user, HT / valid_user, total_loss / valid_user\n",
        "\n",
        "def evaluate_valid(model,dataset,args):\n",
        "  return share_eval(model,dataset,args,evaluate_test=False,criterion=bpr_criterion)\n",
        "\n",
        "def evaluate(model,dataset,args):\n",
        "  return share_eval(model,dataset,args,evaluate_test=True,criterion=bpr_criterion)\n",
        "\n",
        "class PeriodicPlotter:\n",
        "  def __init__(self, sec, xlabel=\"\", ylabel=\"\", scale=None):\n",
        "    self.xlabel = xlabel\n",
        "    self.ylabel = ylabel\n",
        "    self.sec = sec\n",
        "    self.scale = scale\n",
        "\n",
        "    self.tic = time.time()\n",
        "\n",
        "  def plot(self, data):\n",
        "    if time.time() - self.tic > self.sec:\n",
        "      plt.cla()\n",
        "\n",
        "      if self.scale is None:\n",
        "          plt.plot(data)\n",
        "      elif self.scale == \"semilogx\":\n",
        "          plt.semilogx(data)\n",
        "      elif self.scale == \"semilogy\":\n",
        "          plt.semilogy(data)\n",
        "      elif self.scale == \"loglog\":\n",
        "          plt.loglog(data)\n",
        "      else:\n",
        "          raise ValueError(\"unrecognized parameter scale {}\".format(self.scale))\n",
        "\n",
        "      plt.xlabel(self.xlabel)\n",
        "      plt.ylabel(self.ylabel)\n",
        "      ipythondisplay.clear_output(wait=True)\n",
        "      ipythondisplay.display(plt.gcf())\n",
        "\n",
        "      self.tic = time.time()\n"
      ],
      "metadata": {
        "id": "yWzfd_Vwq1en"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "args=dict(\n",
        "    d_model=32,\n",
        "    n_head=2,\n",
        "    d_ff=4*32,\n",
        "    n_sublayer=4,\n",
        "    dropout_rate=0.0,\n",
        "\n",
        "    # device='cuda:0',\n",
        "    device='cpu',\n",
        "    inference_only=False,\n",
        "    n_epoch=20,\n",
        "    batch_size=64,\n",
        "    max_len=200,\n",
        "    # l2_emb=1e-4,\n",
        "    l2_emb=0.0,\n",
        "    lr=1e-4,\n",
        "    n_neg_sample=1,   # Use 1 negative sample first\n",
        "\n",
        "    top_k=10\n",
        ")\n",
        "args = argparse.Namespace(**args)"
      ],
      "metadata": {
        "id": "ZTE0D2j7yKWt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "file_url = \"https://raw.githubusercontent.com/pmixer/SASRec.pytorch/bdb6b77da98b9d6c395283d8b4e1a8bd7cb91efa/python/data/ml-1m.txt\"\n",
        "download_from_github(file_url)\n",
        "user_hist=read_file()\n",
        "\n",
        "# Data processing and preparing part\n",
        "\n",
        "# Not actually needed. This dataset has processed in \"User Item\" form with continuous int id\n",
        "user2idx, idx2user = create_user_mapping(user_hist)\n",
        "item2idx, idx2item = create_item_mapping(user_hist)\n",
        "user_hist = make_consecutive_user_hist(user_hist, user2idx, item2idx)\n",
        "n_user = len(user2idx)\n",
        "n_item = len(item2idx)\n",
        "\n",
        "user_train,user_valid,user_test = data_partition(user_hist)\n",
        "dataset = SASRecDataset(user_train, n_user, n_item, args.max_len)\n",
        "dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "cc = 0.0\n",
        "for u in user_train:\n",
        "    cc += len(user_train[u])\n",
        "print('average sequence length: %.2f' % (cc / len(user_train)))\n",
        "\n",
        "print(\"n_item:\", n_item)\n",
        "print(\"max item id in train:\", max([max(seq) for seq in user_train.values()]))\n",
        "print(\"min item id in train:\", min([min(seq) for seq in user_train.values()]))\n",
        "\n",
        "model=SASRec(n_user,n_item,args).to(args.device)\n",
        "model.xavier_normal_init_parameters()\n",
        "\n",
        "if args.inference_only:\n",
        "  model.eval()\n",
        "  t_test=evaluate(model,dataset,args)\n",
        "  print('test (NDCG@10: %.4f, HR@10: %.4f)' % (t_test[0], t_test[1]))\n",
        "\n",
        "model.train()\n",
        "\n",
        "\n",
        "# Train part\n",
        "bpr_criterion=BPRLoss()\n",
        "lion_optimizer=Lion(model.parameters(),lr=args.lr,betas=(0.9,0.99),weight_decay=0.0)\n",
        "\n",
        "best_val_ndcg, best_val_hr=0.0, 0.0\n",
        "\n",
        "history=defaultdict(list)\n",
        "# plotter = PeriodicPlotter(sec=2, xlabel='Iterations', ylabel='Loss')\n",
        "# plt.ion()\n",
        "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
        "\n",
        "experiment=create_experiment(args)\n",
        "for epoch in range(args.n_epoch):\n",
        "  if args.inference_only: break\n",
        "  step_loss=[]\n",
        "  total_loss=0.0\n",
        "  t0=time.time()\n",
        "  for idx,batch in enumerate(tqdm(dataloader)):\n",
        "    lion_optimizer.zero_grad()\n",
        "    u,seq,pos,neg=(i.to(args.device) for i in batch)\n",
        "    pos_logits,neg_logits=model(seq,pos,neg)\n",
        "    mask=(pos!=0)\n",
        "    loss=bpr_criterion(pos_logits[mask],neg_logits[mask])\n",
        "    emb=model.item_emb.weight\n",
        "    loss=loss+args.l2_emb*torch.sum(emb**2)  # Change, use sum of squares\n",
        "    loss.backward()\n",
        "    lion_optimizer.step()\n",
        "    step_loss.append(loss.item())\n",
        "    total_loss += loss.item()\n",
        "    global_step=epoch*len(dataloader)+idx\n",
        "    experiment.log_metric(\"train_loss\",loss.item(),step=global_step)\n",
        "\n",
        "  # Evaluate part\n",
        "  if epoch%2==0:\n",
        "    model.eval()\n",
        "    t1=time.time()-t0\n",
        "    train_loss=total_loss/len(dataloader)\n",
        "    t_valid=evaluate_valid(model,data_partition(user_hist),args)\n",
        "    print('epoch:%d, time: %f(s), train_loss: %.4f, val_loss: %.4f \\n \\\n",
        "        valid (NDCG@10: %.4f, HR@10: %.4f)'\n",
        "        % (epoch, t1, train_loss, t_valid[2], t_valid[0], t_valid[1]))\n",
        "\n",
        "  experiment.log_metric(\"val_loss\", t_valid[2], step=epoch)\n",
        "  experiment.log_metric(\"val_ndcg@10\", t_valid[0], step=epoch)\n",
        "  experiment.log_metric(\"val_hr@10\", t_valid[1], step=epoch)\n",
        "\n",
        "  history[\"train_loss_epoch\"].append(train_loss)\n",
        "  history[\"val_loss\"].append(t_valid[2])\n",
        "  history[\"ndcg\"].append(t_valid[0])\n",
        "  history[\"hr\"].append(t_valid[1])\n",
        "\n",
        "  # plotter.plot(history)\n",
        "\n",
        "  # ax1.clear()\n",
        "  # ax1.plot(history[\"train_loss_epoch\"], label=\"Train Loss (epoch)\")\n",
        "  # ax1.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
        "  # ax1.set_xlabel(\"Epoch\")\n",
        "  # ax1.set_ylabel(\"Loss\")\n",
        "  # ax1.legend()\n",
        "  # ax1.set_title(\"Loss Curve\")\n",
        "\n",
        "  # ax2.clear()\n",
        "  # ax2.plot(history[\"ndcg\"], label=\"NDCG@10\")\n",
        "  # ax2.plot(history[\"hr\"], label=\"HR@10\")\n",
        "  # ax2.set_xlabel(\"Epoch\")\n",
        "  # ax2.set_ylabel(\"Metric\")\n",
        "  # ax2.legend()\n",
        "  # ax2.set_title(\"Validation Metrics\")\n",
        "\n",
        "  # plt.pause(0.01)  # 关键：刷新图像\n",
        "  model.train()\n",
        "# plt.ioff()\n",
        "# plt.show()\n",
        "\n",
        "t_test = evaluate(model, data_partition(user_hist), args)\n",
        "print('test_loss: %.4f valid (NDCG@10: %.4f, HR@10: %.4f)'\n",
        "      % (t_test[2], t_test[0], t_test[1]))\n",
        "\n",
        "experiment.log_metric(\"test_loss\", t_test[2])\n",
        "experiment.log_metric(\"test_ndcg@10\", t_test[0])\n",
        "experiment.log_metric(\"test_hr@10\", t_test[1])\n",
        "\n",
        "# Cleanup stage\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "experiment.log_model(\"sasrec\", \"model.pth\")\n",
        "experiment.end()"
      ],
      "metadata": {
        "id": "aA5M8yvYGr4X",
        "outputId": "79c0e082-0473-4cae-d0b6-edef624460ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "文件已存在：ml-1m.txt，跳过下载\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average sequence length: 163.50\n",
            "n_item: 3416\n",
            "max item id in train: 3416\n",
            "min item id in train: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : rear_waterfall_8322\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/richard-murphy/sasrec-practice/1a1d9033b174405f9a7f6e4738d8fc62\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [1067] : (0.19447576999664307, 0.8163638710975647)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_hr@10 [11]    : (0.003642384105960265, 0.01639072847682119)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [11]     : (0.2747129446132238, 0.47251375803094825)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_ndcg@10 [11]  : (0.0013327579686284604, 0.006728306753769648)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=https%3A%2F%2Fgithub.com%2Fihatethinkingname%2Fdl_practice%2Fblob%2Fmain%2Fsasrec_practice.ipynb\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size     : 64\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     d_ff           : 128\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     d_model        : 32\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device         : cpu\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate   : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inference_only : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     l2_emb         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr             : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_len        : 200\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_epoch        : 20\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_head         : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_neg_sample   : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_sublayer     : 4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_k          : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/richard-murphy/sasrec-practice/866278c248994683ac108d50d45847c0\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 21 metrics, params and output messages\n",
            "100%|██████████| 95/95 [01:09<00:00,  1.37it/s]\n",
            "/tmp/ipython-input-3612435736.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  pos_logits = torch.tensor(pos_logits, device=args.device)\n",
            "/tmp/ipython-input-3612435736.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  neg_logits = torch.tensor(neg_logits, device=args.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:0, time: 69.448402(s), train_loss: 0.5926, val_loss: 0.4719 \n",
            "         valid (NDCG@10: 0.1949, HR@10: 0.3889)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:11<00:00,  1.32it/s]\n",
            "100%|██████████| 95/95 [01:08<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:2, time: 68.858598(s), train_loss: 0.3862, val_loss: 0.4520 \n",
            "         valid (NDCG@10: 0.2317, HR@10: 0.4328)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:10<00:00,  1.34it/s]\n",
            "100%|██████████| 95/95 [01:08<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:4, time: 68.739664(s), train_loss: 0.3606, val_loss: 0.4148 \n",
            "         valid (NDCG@10: 0.2399, HR@10: 0.4421)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:09<00:00,  1.36it/s]\n",
            "100%|██████████| 95/95 [01:09<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:6, time: 69.960523(s), train_loss: 0.3134, val_loss: 0.3549 \n",
            "         valid (NDCG@10: 0.2846, HR@10: 0.5242)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:08<00:00,  1.39it/s]\n",
            "100%|██████████| 95/95 [01:09<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:8, time: 69.448755(s), train_loss: 0.2573, val_loss: 0.3085 \n",
            "         valid (NDCG@10: 0.3382, HR@10: 0.5907)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:09<00:00,  1.36it/s]\n",
            "100%|██████████| 95/95 [01:09<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:10, time: 69.063577(s), train_loss: 0.2217, val_loss: 0.2745 \n",
            "         valid (NDCG@10: 0.3887, HR@10: 0.6517)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:08<00:00,  1.38it/s]\n",
            "100%|██████████| 95/95 [01:09<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:12, time: 69.810419(s), train_loss: 0.1951, val_loss: 0.2500 \n",
            "         valid (NDCG@10: 0.4230, HR@10: 0.6934)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:09<00:00,  1.37it/s]\n",
            "100%|██████████| 95/95 [01:08<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:14, time: 68.230572(s), train_loss: 0.1785, val_loss: 0.2349 \n",
            "         valid (NDCG@10: 0.4394, HR@10: 0.7124)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:07<00:00,  1.40it/s]\n",
            "100%|██████████| 95/95 [01:08<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:16, time: 68.767378(s), train_loss: 0.1645, val_loss: 0.2227 \n",
            "         valid (NDCG@10: 0.4633, HR@10: 0.7382)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:10<00:00,  1.36it/s]\n",
            "100%|██████████| 95/95 [01:08<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................epoch:18, time: 68.973686(s), train_loss: 0.1546, val_loss: 0.2143 \n",
            "         valid (NDCG@10: 0.4813, HR@10: 0.7515)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 95/95 [01:08<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "............................................................test_loss: 0.0679 valid (NDCG@10: 0.7530, HR@10: 0.9419)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : mammoth_barbel_9946\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/richard-murphy/sasrec-practice/866278c248994683ac108d50d45847c0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_hr@10        : 0.9418874172185431\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_loss         : 0.06786270452035353\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     test_ndcg@10      : 0.7530242471846877\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [1900] : (0.12474580854177475, 0.8332055807113647)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_hr@10 [20]    : (0.3889072847682119, 0.7514900662251656)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_loss [20]     : (0.21431200382748228, 0.47189702849514437)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_ndcg@10 [20]  : (0.19492411653175926, 0.48131075410981344)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=https%3A%2F%2Fgithub.com%2Fihatethinkingname%2Fdl_practice%2Fblob%2Fmain%2Fsasrec_practice.ipynb\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size     : 64\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     d_ff           : 128\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     d_model        : 32\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device         : cpu\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout_rate   : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     inference_only : False\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     l2_emb         : 0.0\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr             : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_len        : 200\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_epoch        : 20\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_head         : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_neg_sample   : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_sublayer     : 4\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     top_k          : 10\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element       : 1 (724.07 KB)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 生成假数据\n",
        "epochs = np.arange(1, 21)\n",
        "train_loss = np.linspace(1.0, 0.3, 20) + np.random.normal(0, 0.05, 20)\n",
        "val_loss = np.linspace(1.2, 0.4, 20) + np.random.normal(0, 0.05, 20)\n",
        "ndcg = np.linspace(0.2, 0.6, 20) + np.random.normal(0, 0.02, 20)\n",
        "hr = np.linspace(0.3, 0.8, 20) + np.random.normal(0, 0.02, 20)\n",
        "\n",
        "# 绘制子图\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# 损失曲线\n",
        "ax1.plot(epochs, train_loss, label=\"Train Loss\")\n",
        "ax1.plot(epochs, val_loss, label=\"Validation Loss\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.set_title(\"Training and Validation Loss\")\n",
        "ax1.legend()\n",
        "\n",
        "# 指标曲线\n",
        "ax2.plot(epochs, ndcg, label=\"NDCG@10\")\n",
        "ax2.plot(epochs, hr, label=\"HR@10\")\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Metric\")\n",
        "ax2.set_title(\"Evaluation Metrics\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "emPLW0xUc4E8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}